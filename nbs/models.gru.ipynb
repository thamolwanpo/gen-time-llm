{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from gen_time_llm.tsdataset import TimeSeriesDataset, TimeSeriesDataModule\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from gen_time_llm.common._base_model import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GRUGPTModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Model combining a GRU encoder for time series and a GPT-based decoder for text generation,\n",
    "    with temporal normalization/scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_seed,\n",
    "        loss,  # Loss function for training\n",
    "        tokenizer,  # Tokenizer for decoding generated text\n",
    "        hidden_size=256,  # Hidden size of the GRU encoder\n",
    "        num_layers=4,  # Number of GRU layers\n",
    "        base_lr=1e-5,  # Learning rate\n",
    "        max_length=512,  # Maximum length of generated sequences\n",
    "        num_beams=3,  # Number of beams for beam search\n",
    "        gru_input_size=128,  # Size of the input for the GRU (e.g., number of features in the time series)\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            random_seed=random_seed,\n",
    "            loss=loss,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # GRU Encoder\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=gru_input_size,  # Number of input features in the time series\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # GPT Decoder\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        # Freeze the GPT model parameters\n",
    "        for param in self.gpt.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Put the GPT model in evaluation mode\n",
    "        self.gpt.eval()  # Ensure GPT is in evaluation mode\n",
    "\n",
    "        # Mapping GRU hidden state to the GPT's embedding size\n",
    "        self.hidden_to_gpt = nn.Linear(hidden_size, self.gpt.config.n_embd)\n",
    "\n",
    "        # Learning rate\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "    def forward(self, batch, targets=None, use_teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        - time_series: Time series input (batch_size, seq_length, num_features)\n",
    "        - targets: Target text used for teacher forcing (optional)\n",
    "        - use_teacher_forcing: Boolean flag for using teacher forcing\n",
    "        Returns:\n",
    "        - gpt_output.loss if using teacher forcing\n",
    "        - gpt_output logits if autoregressive generation\n",
    "        \"\"\"\n",
    "        inputs = {key: batch[key] for key in self.input_keys}\n",
    "        time_series = inputs['temporal_series']\n",
    "\n",
    "        # GRU encoding\n",
    "        _, hidden_state = self.gru(time_series)\n",
    "        hidden_state = hidden_state[-1]\n",
    "\n",
    "        # Map hidden state to GPT's input size (this is the time series representation)\n",
    "        gpt_input = self.hidden_to_gpt(hidden_state).unsqueeze(1)  # (batch_size, 1, gpt_hidden_size)\n",
    "\n",
    "        if use_teacher_forcing and targets is not None:\n",
    "            # Teacher forcing: pass inputs and labels to GPT for loss computation\n",
    "            gpt_input_ids = targets[:, :-1]  # Input part of the target sequence (ignore last token)\n",
    "            token_embeddings = self.gpt.transformer.wte(gpt_input_ids)\n",
    "            gpt_input_combined = torch.cat([gpt_input, token_embeddings], dim=1)\n",
    "            \n",
    "            # Pass to GPT and compute loss directly\n",
    "            gpt_output = self.gpt(inputs_embeds=gpt_input_combined, labels=targets)\n",
    "            return gpt_output.loss\n",
    "\n",
    "        else:\n",
    "            # Autoregressive generation with past_key_values management\n",
    "            outputs = []\n",
    "\n",
    "            for _ in range(self.max_length):\n",
    "                # Generate the next token with time series embedding (gpt_input) and past_key_values\n",
    "                gpt_output = self.gpt(inputs_embeds=gpt_input)\n",
    "                logits = gpt_output.logits[:, -1, :]\n",
    "                outputs.append(logits.unsqueeze(1))\n",
    "\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "                gpt_input = self.gpt.transformer.wte(next_token).unsqueeze(1)\n",
    "\n",
    "            outputs = torch.cat(outputs, dim=1)  # Concatenate the outputs along sequence dimension\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, time_series, max_length=None, num_beams=3):\n",
    "      \"\"\"\n",
    "      Generate text from time series data using autoregressive generation and beam search.\n",
    "      \"\"\"\n",
    "      max_length = max_length if max_length is not None else self.max_length\n",
    "\n",
    "      # GRU encoding\n",
    "      _, hidden_state = self.gru(time_series)\n",
    "      hidden_state = hidden_state[-1]  # Get the last layer's hidden state (batch_size, hidden_size)\n",
    "\n",
    "      # Map hidden state to GPT's input size\n",
    "      gpt_input = self.hidden_to_gpt(hidden_state)\n",
    "\n",
    "      # Start token (e.g., <BOS> or a special token depending on your tokenizer)\n",
    "      start_token = self.tokenizer.bos_token_id if self.tokenizer.bos_token_id else self.tokenizer.eos_token_id\n",
    "      input_ids = torch.full((time_series.size(0), 1), start_token, dtype=torch.long, device=self.device)\n",
    "\n",
    "      # Pass the hidden state to GPT as embeddings (initial hidden state for generation)\n",
    "      inputs_embeds = gpt_input.unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "      # Generate text using the built-in generate() function from transformers\n",
    "      generated_ids = self.gpt.generate(\n",
    "          inputs_embeds=inputs_embeds,  # Use the hidden state as input embeddings\n",
    "          max_length=max_length,\n",
    "          num_beams=num_beams,\n",
    "          early_stopping=True,  # Stop when all beams generate the <EOS> token\n",
    "          no_repeat_ngram_size=2,  # Optionally prevent repetition\n",
    "      )\n",
    "\n",
    "      # Decode the generated token IDs back into text\n",
    "      generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "      return generated_text\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.base_lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### GRUGPTModel\n",
       "\n",
       ">      GRUGPTModel (random_seed, loss, tokenizer, hidden_size=256, num_layers=4,\n",
       ">                   base_lr=1e-05, max_length=512, num_beams=3,\n",
       ">                   gru_input_size=128, **kwargs)\n",
       "\n",
       "*Model combining a GRU encoder for time series and a GPT-based decoder for text generation,\n",
       "with temporal normalization/scaling.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| random_seed |  |  |  |\n",
       "| loss |  |  | Loss function for training |\n",
       "| tokenizer |  |  | Tokenizer for decoding generated text |\n",
       "| hidden_size | int | 256 | Hidden size of the GRU encoder |\n",
       "| num_layers | int | 4 | Number of GRU layers |\n",
       "| base_lr | float | 1e-05 | Learning rate |\n",
       "| max_length | int | 512 | Maximum length of generated sequences |\n",
       "| num_beams | int | 3 | Number of beams for beam search |\n",
       "| gru_input_size | int | 128 | Size of the input for the GRU (e.g., number of features in the time series) |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### GRUGPTModel\n",
       "\n",
       ">      GRUGPTModel (random_seed, loss, tokenizer, hidden_size=256, num_layers=4,\n",
       ">                   base_lr=1e-05, max_length=512, num_beams=3,\n",
       ">                   gru_input_size=128, **kwargs)\n",
       "\n",
       "*Model combining a GRU encoder for time series and a GPT-based decoder for text generation,\n",
       "with temporal normalization/scaling.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| random_seed |  |  |  |\n",
       "| loss |  |  | Loss function for training |\n",
       "| tokenizer |  |  | Tokenizer for decoding generated text |\n",
       "| hidden_size | int | 256 | Hidden size of the GRU encoder |\n",
       "| num_layers | int | 4 | Number of GRU layers |\n",
       "| base_lr | float | 1e-05 | Learning rate |\n",
       "| max_length | int | 512 | Maximum length of generated sequences |\n",
       "| num_beams | int | 3 | Number of beams for beam search |\n",
       "| gru_input_size | int | 128 | Size of the input for the GRU (e.g., number of features in the time series) |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GRUGPTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def objective(trial: Trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function to tune the hyperparameters for the GRUGPTModel.\n",
    "    \"\"\"\n",
    "    # Hyperparameter search space\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256, step=64)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # Instantiate the model with the trial's hyperparameters\n",
    "    model = GRUGPTModel(\n",
    "        input_keys=['temporal_series'],\n",
    "        gru_input_size=218,\n",
    "        random_seed=42,\n",
    "        loss=torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id),\n",
    "        tokenizer=tokenizer,  # Assume you have a tokenizer loaded\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        base_lr=learning_rate\n",
    "    )\n",
    "\n",
    "    # Define a PyTorch Lightning Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=5,\n",
    "        logger=TensorBoardLogger(\"lightning_logs/\", name=\"optuna_gru_gpt\"),\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")],\n",
    "    )\n",
    "\n",
    "    dataset = TimeSeriesDataset.from_jsonl('data/processed_train_data.jsonl', tokenizer=tokenizer)\n",
    "\n",
    "    # Assuming dataset is an instance of your TimeSeriesDataset\n",
    "    total_size = len(dataset)\n",
    "\n",
    "    # Define the lengths for training and validation sets\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create a TimeSeriesDataModule using the given datasets and tokenizer\n",
    "    datamodule = TimeSeriesDataModule(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,  # Use the same tokenizer\n",
    "        batch_size=4,\n",
    "        valid_batch_size=4\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    # Return validation loss for Optuna to optimize\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "\n",
    "# # Running the Optuna study\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "Seed set to 42\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "Generated Text:  ['.\\n\\n\"I\\'m not going to say it\\'s a bad thing,\" he said. \"It\\'s just that I think it would be nice to have a little bit more freedom to do what I want to. I don\\'t know if']\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# Step 1: Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 2: Instantiate the model\n",
    "model = GRUGPTModel(\n",
    "    random_seed=42,\n",
    "    loss=torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id),  # Loss is not needed for inference but required for initialization\n",
    "    tokenizer=tokenizer,\n",
    "    hidden_size=256,  # Use the same hidden_size as in your trained model\n",
    "    num_layers=4,  # Same number of layers as in your trained model\n",
    "    gru_input_size=218,  # Adjust this based on your input features\n",
    ")\n",
    "\n",
    "# Step 3: Load pre-trained weights if you have them\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "dataset = TimeSeriesDataset.from_jsonl('data/processed_train_data.jsonl', tokenizer=tokenizer)\n",
    "\n",
    "# Step 5: Use the `generate` method to generate text\n",
    "generated_text = model.generate(time_series=dataset[0]['temporal_series'].unsqueeze(0), max_length=50, num_beams=3)\n",
    "\n",
    "# Step 6: Print the generated text\n",
    "print(\"Generated Text: \", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
