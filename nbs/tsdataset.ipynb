{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset/Loader\n",
    "> Torch Dataset for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from gen_time_llm.utils import generate_fake_data\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from collections.abc import Mapping\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LengthBasedBatchSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size, sort_key='summary_input_ids'):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_key = sort_key\n",
    "\n",
    "        # Sort indices by the length of `sort_key`\n",
    "        self.sorted_indices = sorted(range(len(data_source)), key=lambda i: len(data_source[i][sort_key]))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Generate batches from sorted indices\n",
    "        batches = [self.sorted_indices[i:i + self.batch_size] for i in range(0, len(self.sorted_indices), self.batch_size)]\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesLoader(DataLoader):\n",
    "    \"\"\"TimeSeriesLoader DataLoader.\n",
    "    \n",
    "    Custom DataLoader to work with time series datasets, handling dynamic padding for tokenized summaries and attention masks,\n",
    "    and using the tokenizer's `eos_token_id` for padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the loader with the dataset and tokenizer.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset: The TimeSeriesDataset instance.\n",
    "        - tokenizer: The tokenizer used for tokenizing summaries (e.g., from HuggingFace's Transformers library).\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer  # Store the tokenizer for eos_token_id\n",
    "        if 'collate_fn' in kwargs:\n",
    "            kwargs.pop('collate_fn')\n",
    "        kwargs_ = {**kwargs, **dict(collate_fn=self._collate_fn)}\n",
    "        super().__init__(dataset=dataset, **kwargs_)\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle time series data and dynamically pad tokenized summaries with `eos_token_id`.\n",
    "        \"\"\"\n",
    "        elem = batch[0]\n",
    "        elem_type = type(elem)\n",
    "\n",
    "        # Handle case when the batch is a tensor (e.g., temporal series)\n",
    "        if isinstance(elem, torch.Tensor):\n",
    "            return torch.stack(batch, dim=0)\n",
    "\n",
    "        # Handle case when the batch is a dictionary\n",
    "        elif isinstance(elem, Mapping):\n",
    "            # Collate temporal series (stack 2D time series tensors)\n",
    "            temporal_series = self.collate_fn([d['temporal_series'] for d in batch])\n",
    "            \n",
    "            # Collate sector information (as a list)\n",
    "            sector = [d['sector'] for d in batch]\n",
    "            \n",
    "            # Find the maximum sequence length in the current batch for dynamic padding\n",
    "            max_length = max([d['summary_input_ids'].size(0) for d in batch])\n",
    "            \n",
    "            # Dynamically pad summaries using eos_token_id\n",
    "            eos_token_id = self.tokenizer.eos_token_id\n",
    "            summary_input_ids = torch.stack([torch.cat([d['summary_input_ids'], \n",
    "                                                        torch.full((max_length - d['summary_input_ids'].size(0),), \n",
    "                                                                   eos_token_id, dtype=torch.long)])\n",
    "                                             for d in batch])\n",
    "            \n",
    "            # Dynamically pad attention masks (using 0 for padding)\n",
    "            attention_mask = None\n",
    "            if batch[0]['attention_mask'] is not None:\n",
    "                attention_mask = torch.stack([torch.cat([d['attention_mask'], \n",
    "                                                         torch.zeros(max_length - d['attention_mask'].size(0), \n",
    "                                                                     dtype=torch.long)])\n",
    "                                              for d in batch])\n",
    "            \n",
    "            # Collate country information (keeping as list of strings)\n",
    "            country = [d['country'] for d in batch]\n",
    "            \n",
    "            # Collate columns of temporal data (should remain consistent across batch)\n",
    "            temporal_cols = batch[0]['temporal_cols']\n",
    "\n",
    "            year_range = [d['year_range'] for d in batch]\n",
    "\n",
    "            # Return the collated batch with dynamic padding for tokenized summaries\n",
    "            return dict(\n",
    "                temporal_series=temporal_series,\n",
    "                sector=sector,\n",
    "                summary_input_ids=summary_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                country=country,\n",
    "                temporal_cols=temporal_cols,\n",
    "                year_range=year_range\n",
    "            )\n",
    "\n",
    "        # Raise error if an unsupported data type is passed\n",
    "        raise TypeError(f'Unknown type {elem_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeSeriesLoader\n",
       "\n",
       ">      TimeSeriesLoader (dataset, tokenizer, **kwargs)\n",
       "\n",
       "*TimeSeriesLoader DataLoader.\n",
       "\n",
       "Custom DataLoader to work with time series datasets, handling dynamic padding for tokenized summaries and attention masks,\n",
       "and using the tokenizer's `eos_token_id` for padding.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeSeriesLoader\n",
       "\n",
       ">      TimeSeriesLoader (dataset, tokenizer, **kwargs)\n",
       "\n",
       "*TimeSeriesLoader DataLoader.\n",
       "\n",
       "Custom DataLoader to work with time series datasets, handling dynamic padding for tokenized summaries and attention masks,\n",
       "and using the tokenizer's `eos_token_id` for padding.*"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeriesLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_list,  # List of dictionaries containing time series and metadata\n",
    "                 tokenizer,  # Tokenizer for summarizing text (e.g., from HuggingFace's Transformers library)\n",
    "                 max_length: int = 512,  # Max token length for tokenization\n",
    "                 sorted=False,  # Whether the dataset is already sorted\n",
    "                 add_attention_mask: bool = True  # Whether to include attention mask for tokenized summaries\n",
    "                ):\n",
    "        \"\"\"\n",
    "        A dataset class for structured time series data, with both temporal and static (text) features.\n",
    "        \n",
    "        Parameters:\n",
    "        - data_list: List of dictionaries, where each dictionary contains keys like:\n",
    "            - 'anchor_summary': Short description or metadata (to be tokenized).\n",
    "            - 'positive_time_series': 2D array of temporal data for the entity.\n",
    "            - 'positive_sector': One-hot encoded sector information.\n",
    "            - 'sector': Sectors assigned to this time series (as string).\n",
    "            - 'country': Country associated with the time series.\n",
    "            - 'columns': Names of the temporal columns/features.\n",
    "        - tokenizer: Tokenizer instance for encoding the summaries (e.g., GPT tokenizer or any other transformer model).\n",
    "        - max_length: Maximum length for the tokenized summaries (default: 512).\n",
    "        - sorted: Whether the dataset is already sorted (default: False).\n",
    "        - add_attention_mask: Whether to add an attention mask for tokenized summaries (default: True).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_list = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sorted = sorted\n",
    "        self.add_attention_mask = add_attention_mask\n",
    "        \n",
    "        # Filter out data entries with tokenized summary lengths < 100\n",
    "        self.data_list = [\n",
    "            data for data in data_list \n",
    "            if len(tokenizer(data['anchor_summary'], max_length=max_length, truncation=True)['input_ids']) >= 100\n",
    "        ]\n",
    "\n",
    "        self.n_groups = len(self.data_list)  # Update the count after filtering\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Remove duplicate spaces and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "        text = text.replace('\\n', ' ')  # Replace newlines with a space\n",
    "        text = text.strip()  # Remove leading and trailing spaces\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of time series entities in the dataset.\n",
    "        \"\"\"\n",
    "        return self.n_groups\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single item from the dataset (time series and its metadata).\n",
    "        The index `idx` specifies which time series entity to retrieve.\n",
    "        \"\"\"\n",
    "        data = self.data_list[idx]\n",
    "        \n",
    "        # Extract fields from the dictionary\n",
    "        temporal_series = torch.tensor(data['positive_time_series'], dtype=torch.float32)\n",
    "        anchor_summary = self.clean_text(data['anchor_summary'])\n",
    "        country = data['country']\n",
    "        columns = data['columns']\n",
    "        sector_str = data['sector']  # This is a string representation of sectors\n",
    "        year_range = data['year_range']\n",
    "\n",
    "        # Manually add the BOS and EOS tokens to the input summary\n",
    "        # bos_token = self.tokenizer.bos_token or self.tokenizer.cls_token  # Default to CLS if BOS isn't defined\n",
    "        eos_token = self.tokenizer.eos_token or self.tokenizer.sep_token  # Default to SEP if EOS isn't defined\n",
    "\n",
    "        # Concatenate the EOS tokens to the summary\n",
    "        anchor_summary_with_eos = anchor_summary + \" \" + eos_token\n",
    "\n",
    "        # Tokenize the summary with the specified tokenizer\n",
    "        tokenized_summary = self.tokenizer(\n",
    "            anchor_summary_with_eos,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "\n",
    "        # Extract tokenized input_ids and attention mask (optional)\n",
    "        input_ids = tokenized_summary['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = tokenized_summary['attention_mask'].squeeze(0) if self.add_attention_mask else None\n",
    "\n",
    "        # Return a dictionary with both temporal and static features, including tokenized summary\n",
    "        return {\n",
    "            'temporal_series': temporal_series,  # 2D time series data\n",
    "            'sector': sector_str,                # Sectors as string\n",
    "            'summary_input_ids': input_ids,      # Tokenized summary\n",
    "            'attention_mask': attention_mask,    # Attention mask (if applicable)\n",
    "            'country': country,                  # Static feature (country)\n",
    "            'temporal_cols': columns,            # Names of temporal features\n",
    "            'year_range': year_range\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the dataset, showing the number of data points and groups.\n",
    "        \"\"\"\n",
    "        return f\"TimeSeriesDataset(n_data={len(self.data_list):,}, n_groups={self.n_groups:,})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Check if two datasets are equal by comparing their data and attributes.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, TimeSeriesDataset):\n",
    "            return False\n",
    "        return (\n",
    "            self.data_list == other.data_list and\n",
    "            self.max_length == other.max_length and\n",
    "            self.sorted == other.sorted\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_jsonl(file_path, tokenizer, max_length=512, sorted=False, add_attention_mask=True):\n",
    "        \"\"\"\n",
    "        Static method to load time series data from a JSONL file.\n",
    "        \n",
    "        Parameters:\n",
    "        - file_path: Path to the JSONL file.\n",
    "        - tokenizer: Tokenizer to use for tokenizing the summaries.\n",
    "        - max_length: Maximum token length for the summaries.\n",
    "        - sorted: Whether the dataset should be sorted.\n",
    "        - add_attention_mask: Whether to include attention masks for tokenized summaries.\n",
    "\n",
    "        Returns:\n",
    "        - dataset: TimeSeriesDataset instance with loaded data.\n",
    "        \"\"\"\n",
    "        # Load the JSONL file\n",
    "        data_list = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data_list.append(json.loads(line))\n",
    "\n",
    "        # Create and return the dataset instance\n",
    "        return TimeSeriesDataset(\n",
    "            data_list=data_list,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_length,\n",
    "            sorted=sorted,\n",
    "            add_attention_mask=add_attention_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeSeriesDataset\n",
       "\n",
       ">      TimeSeriesDataset (data_list, tokenizer, max_length:int=512,\n",
       ">                         sorted=False, add_attention_mask:bool=True)\n",
       "\n",
       "*An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`. Subclasses could also\n",
       "optionally implement :meth:`__getitems__`, for speedup batched samples\n",
       "loading. This method accepts list of indices of samples of batch and returns\n",
       "list of samples.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs an index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data_list |  |  | List of dictionaries containing time series and metadata |\n",
       "| tokenizer |  |  | Tokenizer for summarizing text (e.g., from HuggingFace's Transformers library) |\n",
       "| max_length | int | 512 | Max token length for tokenization |\n",
       "| sorted | bool | False | Whether the dataset is already sorted |\n",
       "| add_attention_mask | bool | True | Whether to include attention mask for tokenized summaries |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeSeriesDataset\n",
       "\n",
       ">      TimeSeriesDataset (data_list, tokenizer, max_length:int=512,\n",
       ">                         sorted=False, add_attention_mask:bool=True)\n",
       "\n",
       "*An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`. Subclasses could also\n",
       "optionally implement :meth:`__getitems__`, for speedup batched samples\n",
       "loading. This method accepts list of indices of samples of batch and returns\n",
       "list of samples.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs an index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data_list |  |  | List of dictionaries containing time series and metadata |\n",
       "| tokenizer |  |  | Tokenizer for summarizing text (e.g., from HuggingFace's Transformers library) |\n",
       "| max_length | int | 512 | Max token length for tokenization |\n",
       "| sorted | bool | False | Whether the dataset is already sorted |\n",
       "| add_attention_mask | bool | True | Whether to include attention mask for tokenized summaries |"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeriesDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesDataset(n_data=1, n_groups=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "synthetic_data = generate_fake_data(n_series=1, n_temporal_features=2, mode='train')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "TimeSeriesDataset(synthetic_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TimeSeriesDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            train_dataset: TimeSeriesDataset,  # Separate dataset for training\n",
    "            val_dataset: TimeSeriesDataset,    # Separate dataset for validation\n",
    "            tokenizer,                         # Tokenizer for all datasets\n",
    "            batch_size=32, \n",
    "            valid_batch_size=8,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            shuffle_train=True,\n",
    "            test_dataset: TimeSeriesDataset = None,   # Separate dataset for testing (optional)\n",
    "        ):\n",
    "        \"\"\"\n",
    "        A DataModule for loading time series data, supporting training, validation, and prediction.\n",
    "        \n",
    "        Parameters:\n",
    "        - train_dataset: The TimeSeriesDataset instance for the training data.\n",
    "        - val_dataset: The TimeSeriesDataset instance for the validation data.\n",
    "        - test_dataset: The TimeSeriesDataset instance for the test data (optional).\n",
    "        - tokenizer: The tokenizer used for tokenizing summaries (e.g., from HuggingFace's Transformers library).\n",
    "        - batch_size: Batch size for the training data.\n",
    "        - valid_batch_size: Batch size for the validation and test data.\n",
    "        - num_workers: Number of workers for data loading (default: 0).\n",
    "        - drop_last: Whether to drop the last incomplete batch (default: False).\n",
    "        - shuffle_train: Whether to shuffle the training data (default: True).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle_train = shuffle_train\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Ensure padding token is set\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Creates and returns a DataLoader for the training dataset.\n",
    "        \"\"\"\n",
    "        # sampler = LengthBasedBatchSampler(self.train_dataset, batch_size=self.batch_size, sort_key='summary_input_ids')\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.train_dataset,\n",
    "            tokenizer=self.tokenizer,  # Pass the tokenizer\n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=self.shuffle_train,\n",
    "            # batch_sampler=sampler,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "        # loader = TimeSeriesLoader(\n",
    "        #     self.train_dataset,\n",
    "        #     tokenizer=self.tokenizer,\n",
    "        #     num_workers=self.num_workers,\n",
    "        #     batch_sampler=sampler\n",
    "        # )\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Creates and returns a DataLoader for the validation dataset.\n",
    "        \"\"\"\n",
    "        loader = TimeSeriesLoader(\n",
    "            self.val_dataset, \n",
    "            tokenizer=self.tokenizer,  # Pass the tokenizer\n",
    "            batch_size=self.valid_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        Creates and returns a DataLoader for the test dataset.\n",
    "        \"\"\"\n",
    "        if self.test_dataset:\n",
    "            loader = TimeSeriesLoader(\n",
    "                self.test_dataset,\n",
    "                tokenizer=self.tokenizer,  # Pass the tokenizer\n",
    "                batch_size=self.valid_batch_size, \n",
    "                num_workers=self.num_workers,\n",
    "                shuffle=False\n",
    "            )\n",
    "            return loader\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeSeriesDataModule\n",
       "\n",
       ">      TimeSeriesDataModule (train_dataset:__main__.TimeSeriesDataset,\n",
       ">                            val_dataset:__main__.TimeSeriesDataset, tokenizer,\n",
       ">                            batch_size=32, valid_batch_size=8, num_workers=0,\n",
       ">                            drop_last=False, shuffle_train=True,\n",
       ">                            test_dataset:__main__.TimeSeriesDataset=None)\n",
       "\n",
       "*A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    import lightning.pytorch as L\n",
       "    import torch.utils.data as data\n",
       "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
       "\n",
       "    class MyDataModule(L.LightningDataModule):\n",
       "        def prepare_data(self):\n",
       "            # download, IO, etc. Useful with shared filesystems\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "            ...\n",
       "\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "            dataset = RandomDataset(1, 100)\n",
       "            self.train, self.val, self.test = data.random_split(\n",
       "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
       "            )\n",
       "\n",
       "        def train_dataloader(self):\n",
       "            return data.DataLoader(self.train)\n",
       "\n",
       "        def val_dataloader(self):\n",
       "            return data.DataLoader(self.val)\n",
       "\n",
       "        def test_dataloader(self):\n",
       "            return data.DataLoader(self.test)\n",
       "\n",
       "        def on_exception(self, exception):\n",
       "            # clean up state after the trainer faced an exception\n",
       "            ...\n",
       "\n",
       "        def teardown(self):\n",
       "            # clean up state after the trainer stops, delete files...\n",
       "            # called on every process in DDP\n",
       "            ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| train_dataset | TimeSeriesDataset |  | Separate dataset for training |\n",
       "| val_dataset | TimeSeriesDataset |  | Separate dataset for validation |\n",
       "| tokenizer |  |  | Tokenizer for all datasets |\n",
       "| batch_size | int | 32 |  |\n",
       "| valid_batch_size | int | 8 |  |\n",
       "| num_workers | int | 0 |  |\n",
       "| drop_last | bool | False |  |\n",
       "| shuffle_train | bool | True |  |\n",
       "| test_dataset | TimeSeriesDataset | None | Separate dataset for testing (optional) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeSeriesDataModule\n",
       "\n",
       ">      TimeSeriesDataModule (train_dataset:__main__.TimeSeriesDataset,\n",
       ">                            val_dataset:__main__.TimeSeriesDataset, tokenizer,\n",
       ">                            batch_size=32, valid_batch_size=8, num_workers=0,\n",
       ">                            drop_last=False, shuffle_train=True,\n",
       ">                            test_dataset:__main__.TimeSeriesDataset=None)\n",
       "\n",
       "*A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    import lightning.pytorch as L\n",
       "    import torch.utils.data as data\n",
       "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
       "\n",
       "    class MyDataModule(L.LightningDataModule):\n",
       "        def prepare_data(self):\n",
       "            # download, IO, etc. Useful with shared filesystems\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "            ...\n",
       "\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "            dataset = RandomDataset(1, 100)\n",
       "            self.train, self.val, self.test = data.random_split(\n",
       "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
       "            )\n",
       "\n",
       "        def train_dataloader(self):\n",
       "            return data.DataLoader(self.train)\n",
       "\n",
       "        def val_dataloader(self):\n",
       "            return data.DataLoader(self.val)\n",
       "\n",
       "        def test_dataloader(self):\n",
       "            return data.DataLoader(self.test)\n",
       "\n",
       "        def on_exception(self, exception):\n",
       "            # clean up state after the trainer faced an exception\n",
       "            ...\n",
       "\n",
       "        def teardown(self):\n",
       "            # clean up state after the trainer stops, delete files...\n",
       "            # called on every process in DDP\n",
       "            ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| train_dataset | TimeSeriesDataset |  | Separate dataset for training |\n",
       "| val_dataset | TimeSeriesDataset |  | Separate dataset for validation |\n",
       "| tokenizer |  |  | Tokenizer for all datasets |\n",
       "| batch_size | int | 32 |  |\n",
       "| valid_batch_size | int | 8 |  |\n",
       "| num_workers | int | 0 |  |\n",
       "| drop_last | bool | False |  |\n",
       "| shuffle_train | bool | True |  |\n",
       "| test_dataset | TimeSeriesDataset | None | Separate dataset for testing (optional) |"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeSeriesDataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary shape:  torch.Size([2, 8])\n",
      "tensor([[28650,  3155,   976,    13, 50256, 50256, 50256, 50256],\n",
      "        [18465,  1657,  4583, 11376,  1808,   649,  3051,    13]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "synthetic_data = generate_fake_data(n_series=10, n_temporal_features=2, mode='train')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "dataset = TimeSeriesDataset(synthetic_data, tokenizer)\n",
    "\n",
    "batch_size = 2\n",
    "data = TimeSeriesDataModule(train_dataset=dataset, val_dataset=dataset, tokenizer=tokenizer,\n",
    "                            batch_size=batch_size, drop_last=True)\n",
    "\n",
    "for batch in data.train_dataloader():\n",
    "    print('summary shape: ', batch['summary_input_ids'].shape)\n",
    "    print(batch['summary_input_ids'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
