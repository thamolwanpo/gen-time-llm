{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    BaseModel for time series to text tasks.\n",
    "    \n",
    "    This base class is designed for models that take time series data as input and generate textual summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_seed,\n",
    "        loss,  # Loss function\n",
    "        valid_loss=None,  # Validation loss (optional)\n",
    "        optimizer=torch.optim.Adam,  # Default optimizer\n",
    "        optimizer_kwargs=None,  # Additional arguments for optimizer\n",
    "        lr_scheduler=torch.optim.lr_scheduler.StepLR,  # Default learning rate scheduler\n",
    "        lr_scheduler_kwargs=None,  # Additional arguments for lr scheduler\n",
    "        max_steps=10000,  # Max number of training steps\n",
    "        early_stop_patience_steps=1000,  # Patience for early stopping\n",
    "        output_key=\"summary_input_ids\",  # Fixed output key for summary\n",
    "        input_keys=None,  # Keys to extract from the batch (dynamically chosen by the model)\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        self.random_seed = random_seed\n",
    "        pl.seed_everything(self.random_seed, workers=True)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = loss\n",
    "        self.valid_loss = valid_loss if valid_loss is not None else loss\n",
    "\n",
    "        # Optimization\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_kwargs = optimizer_kwargs if optimizer_kwargs is not None else {}\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_scheduler_kwargs = lr_scheduler_kwargs if lr_scheduler_kwargs is not None else {}\n",
    "\n",
    "        # Input and output keys\n",
    "        self.output_key = output_key  # Summary key (fixed)\n",
    "        self.input_keys = input_keys if input_keys is not None else []  # Dynamic input keys\n",
    "\n",
    "        # Trainer configuration\n",
    "        self.max_steps = max_steps\n",
    "        self.early_stop_patience_steps = early_stop_patience_steps\n",
    "        self.trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        # Add early stopping\n",
    "        if early_stop_patience_steps > 0:\n",
    "            if \"callbacks\" not in trainer_kwargs:\n",
    "                trainer_kwargs[\"callbacks\"] = []\n",
    "            trainer_kwargs[\"callbacks\"].append(\n",
    "                EarlyStopping(monitor=\"val_loss\", patience=early_stop_patience_steps)\n",
    "            )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Models should implement their custom forward logic using the `input_keys` to select specific inputs from the batch.\n",
    "        \"\"\"\n",
    "        # Extract inputs based on specified keys\n",
    "        inputs = {key: batch[key] for key in self.input_keys}\n",
    "        # Models will implement their forward pass logic using these inputs\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step: compute loss for a single batch.\n",
    "        \"\"\"\n",
    "        target = batch[self.output_key]\n",
    "\n",
    "        # Decide whether to use teacher forcing based on a random threshold\n",
    "        use_teacher_forcing = torch.rand(1).item() < 0.8\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: GPT computes the loss internally\n",
    "            loss = self(batch, targets=target, use_teacher_forcing=True)\n",
    "        else:\n",
    "            # Autoregressive generation: GPT returns token IDs, we need to compute the loss manually\n",
    "            output = self(batch, use_teacher_forcing=False)\n",
    "\n",
    "            # We need to compare the generated output (token IDs) with the target token IDs.\n",
    "            # Since output from `generate()` is token IDs, no need to reshape\n",
    "            target = target[:, 1:]  # Shift target to ignore the first token (for autoregressive prediction)\n",
    "\n",
    "            # Compute the loss manually by comparing output tokens with target tokens\n",
    "            loss = self.loss(output[:, :target.size(1)], target)\n",
    "\n",
    "        # Log the loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step: compute validation loss for a single batch.\n",
    "        \"\"\"\n",
    "        target = batch[self.output_key]\n",
    "\n",
    "        # In validation, we typically use autoregressive generation\n",
    "        output = self(batch, use_teacher_forcing=False)\n",
    "\n",
    "        # We need to compare the generated output (token IDs) with the target token IDs.\n",
    "        # Since output from `generate()` is token IDs, no need to reshape\n",
    "        target = target[:, 1:]  # Shift target to ignore the first token (for autoregressive prediction)\n",
    "\n",
    "        # Compute validation loss manually\n",
    "        val_loss = self.valid_loss(output[:, :target.size(1)], target)\n",
    "\n",
    "        # Log the validation loss\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure the optimizer and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizer(params=self.parameters(), **self.optimizer_kwargs)\n",
    "\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": self.lr_scheduler(optimizer=optimizer, **self.lr_scheduler_kwargs),\n",
    "            \"monitor\": \"val_loss\",  # Monitor validation loss\n",
    "            \"interval\": \"step\",  # Step-based scheduler\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return type(self).__name__\n",
    "\n",
    "    def _restart_seed(self, random_seed):\n",
    "        \"\"\"\n",
    "        Helper method to restart the random seed.\n",
    "        \"\"\"\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_seed\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"\n",
    "        Method called at the start of training to set random seeds.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
