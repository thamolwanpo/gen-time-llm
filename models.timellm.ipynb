{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.timellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-LLM\n",
    "Time-LLM is a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. In other words, it transforms a forecasting task into a \"language task\" that can be tackled by an off-the-shelf LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import optuna\n",
    "import math\n",
    "from optuna.trial import Trial\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from gen_time_llm.common._base_model import BaseModel\n",
    "from gen_time_llm.common._modules import RevIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ReplicationPad1d(nn.Module):\n",
    "    \"\"\"\n",
    "    ReplicationPad1d\n",
    "    \"\"\"       \n",
    "    def __init__(self, padding):\n",
    "        super(ReplicationPad1d, self).__init__()\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input):\n",
    "        replicate_padding = input[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])\n",
    "        output = torch.cat([input, replicate_padding], dim=-1)\n",
    "        return output\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    TokenEmbedding\n",
    "    \"\"\"       \n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbedding\n",
    "    \"\"\"      \n",
    "    def __init__(self, d_model, patch_len, stride, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch_layer = ReplicationPad1d((0, stride))\n",
    "\n",
    "        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        self.value_embedding = TokenEmbedding(patch_len, d_model)\n",
    "\n",
    "        # Positional embedding\n",
    "        # self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do patching\n",
    "        n_vars = x.shape[1]\n",
    "        x = self.padding_patch_layer(x)\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # Input encoding\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x), n_vars\n",
    "    \n",
    "class FlattenHead(nn.Module):\n",
    "    \"\"\"\n",
    "    FlattenHead\n",
    "    \"\"\"       \n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ReprogrammingLayer\n",
    "    \"\"\"       \n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / math.sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TimeLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLLM(BaseModel):\n",
    "\n",
    "    \"\"\" TimeLLM\n",
    "\n",
    "    Time-LLM is a reprogramming framework to repurpose an off-the-shelf LLM for time series forecasting.\n",
    "\n",
    "    It trains a reprogramming layer that translates the observed series into a language task. This is fed to the LLM and an output\n",
    "    projection layer translates the output back to numerical predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_seed,\n",
    "        input_size,\n",
    "        patch_len: int = 4,\n",
    "        stride: int = 2,\n",
    "        d_ff: int = 128,\n",
    "        top_k: int = 5,\n",
    "        d_llm: int = 768,\n",
    "        d_model: int = 32,\n",
    "        n_heads: int = 8,\n",
    "        enc_in: int = 7,\n",
    "        dec_in: int  = 7,\n",
    "        llm = None,\n",
    "        llm_config = None,\n",
    "        llm_tokenizer = None,\n",
    "        llm_num_hidden_layers = 32,\n",
    "        llm_output_attention: bool = True,\n",
    "        llm_output_hidden_states: bool = True,\n",
    "        dropout=0.1,\n",
    "        base_lr=1e-5,  # Learning rate\n",
    "        max_length=512,  # Maximum length of generated sequences\n",
    "        num_beams=3,  # Number of beams for beam search\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            random_seed=random_seed,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.d_ff = d_ff\n",
    "        self.top_k = top_k\n",
    "        self.d_llm = d_llm\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.n_heads = n_heads\n",
    "        self.enc_in = enc_in\n",
    "        self.dec_in = dec_in\n",
    "\n",
    "        DEFAULT_MODEL = \"openai-community/gpt2\"\n",
    "\n",
    "        if llm is None:\n",
    "            print(f\"Using {DEFAULT_MODEL} as default.\")\n",
    "            model_name = DEFAULT_MODEL\n",
    "        else:\n",
    "            model_name = llm\n",
    "\n",
    "        if llm_config is not None or llm_tokenizer is not None:\n",
    "            warnings.warn(\"'llm_config' and 'llm_tokenizer' parameters are deprecated and will be ignored. \"\n",
    "                        \"The config and tokenizer will be automatically loaded from the specified model.\", \n",
    "                        DeprecationWarning)\n",
    "\n",
    "        try:\n",
    "            self.llm_config = AutoConfig.from_pretrained(model_name)\n",
    "            self.llm = AutoModel.from_pretrained(model_name, config=self.llm_config)\n",
    "            self.llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.llm_head = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "            print(f\"Successfully loaded model: {model_name}\")\n",
    "        except EnvironmentError:\n",
    "            print(f\"Failed to load {model_name}. Loading the default model ({DEFAULT_MODEL})...\")\n",
    "            self.llm_config = AutoConfig.from_pretrained(DEFAULT_MODEL)\n",
    "            self.llm = AutoModel.from_pretrained(DEFAULT_MODEL, config=self.llm_config)\n",
    "            self.llm_tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)\n",
    "\n",
    "        self.llm_num_hidden_layers = llm_num_hidden_layers\n",
    "        self.llm_output_attention = llm_output_attention\n",
    "        self.llm_output_hidden_states = llm_output_hidden_states\n",
    "\n",
    "        if self.llm_tokenizer.eos_token:\n",
    "            self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.llm_tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.llm_tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            self.d_model, self.patch_len, self.stride, self.dropout)\n",
    "        \n",
    "        self.word_embeddings = self.llm.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1024\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(self.d_model, self.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "        self.patch_nums = int((input_size - self.patch_len) / self.stride + 2)\n",
    "        self.normalize_layers = RevIN(self.enc_in, affine=False)\n",
    "\n",
    "    def select_top_features_by_variance(self, time_series, top_k=20):\n",
    "        # time_series is assumed to be of shape (B, T, N)\n",
    "        # Compute variance for each feature over time (dim=1 -> T)\n",
    "        feature_variances = torch.var(time_series, dim=1).mean(dim=0)  # Mean variance per feature across batches\n",
    "        \n",
    "        # Get indices of the top `top_k` features by variance\n",
    "        top_features = torch.topk(feature_variances, top_k).indices\n",
    "        return top_features\n",
    "\n",
    "    def encode(self, time_series, country, sector, columns):\n",
    "        x_enc = self.normalize_layers(time_series, 'norm')\n",
    "\n",
    "        # Select top 10 important features based on variance\n",
    "        selected_features = self.select_top_features_by_variance(x_enc, top_k=10)\n",
    "\n",
    "        # Select the corresponding column names for the selected features\n",
    "        selected_columns = [columns[i] for i in selected_features.tolist()]\n",
    "\n",
    "        # Select only the top 10 important features\n",
    "        x_enc = x_enc[:, :, selected_features]  # Shape will be (B, T, 10)\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]  # Min over time (T) for each feature (N)\n",
    "        max_values = torch.max(x_enc, dim=1)[0]  # Max over time (T) for each feature (N)\n",
    "        medians = torch.median(x_enc, dim=1).values  # Median over time (T) for each feature (N)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)  # Sum of differences over time (T) for each feature (N)\n",
    "\n",
    "        prompt = []\n",
    "        for b in range(B):\n",
    "            feature_prompts = []\n",
    "            for n in range(N):\n",
    "                min_values_str = str(min_values[b, n].item())  # No need for flattening now\n",
    "                max_values_str = str(max_values[b, n].item())\n",
    "                median_values_str = str(medians[b, n].item())\n",
    "                trend_str = 'upward' if trends[b, n] > 0 else 'downward'\n",
    "\n",
    "                feature_prompt = (\n",
    "                    f\"Feature {columns[n]} statistics: \"\n",
    "                    f\"min value {min_values_str}, \"\n",
    "                    f\"max value {max_values_str}, \"\n",
    "                    f\"median value {median_values_str}, \"\n",
    "                    f\"the trend is {trend_str}\"\n",
    "                )\n",
    "\n",
    "                feature_prompts.append(feature_prompt)\n",
    "\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Focused country: {country[b]}, sectors: {', '.join(sector[b])} \"\n",
    "                f\"Task description: generate climate policy summary according to the given information; \"\n",
    "                f\"{' '.join(feature_prompts)}<||>\"\n",
    "            )\n",
    "            \n",
    "            prompt.append(prompt_)\n",
    "\n",
    "        prompt = self.llm_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc.to(torch.float32))\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        H_enc = enc_out.size(2)\n",
    "        enc_out = enc_out.view(B, -1, H_enc)  # torch.Size([4, 50, 768])\n",
    "        llm_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "\n",
    "        return llm_enc_out\n",
    "\n",
    "\n",
    "    def forward(self, batch, target):\n",
    "        output = self.encode(batch['temporal_series'], batch['country'], batch['sector'], batch['temporal_cols'])\n",
    "        dec_out = self.llm_head(inputs_embeds=output).logits\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.llm_tokenizer.eos_token_id)\n",
    "\n",
    "        # Check the size of the generated output and target\n",
    "        output_length = dec_out.size(1)\n",
    "        target_length = target.size(1)\n",
    "\n",
    "        # Case 1: If generated output is longer, trim the output\n",
    "        if output_length > target_length:\n",
    "            dec_out = dec_out[:, :target_length, :]\n",
    "        # Case 2: If target is longer, pad the output to match the target length\n",
    "        elif output_length < target_length:\n",
    "            padding_size = target_length - output_length\n",
    "            padding = torch.zeros(dec_out.size(0), padding_size, dec_out.size(2), device=dec_out.device)\n",
    "            dec_out = torch.cat([dec_out, padding], dim=1)\n",
    "\n",
    "        # Reshape output and target for CrossEntropyLoss\n",
    "        dec_out = dec_out.reshape(-1, dec_out.size(-1))  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "        target = target.reshape(-1)  # Reshape to (batch_size * seq_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(dec_out, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using openai-community/gpt2 as default.\n",
      "Successfully loaded model: openai-community/gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | llm                 | GPT2Model          | 124 M  | eval \n",
      "1 | llm_head            | GPT2LMHeadModel    | 124 M  | eval \n",
      "2 | patch_embedding     | PatchEmbedding     | 384    | train\n",
      "3 | mapping_layer       | Linear             | 51.5 M | train\n",
      "4 | reprogramming_layer | ReprogrammingLayer | 2.4 M  | train\n",
      "5 | normalize_layers    | RevIN              | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "178 M     Trainable params\n",
      "124 M     Non-trainable params\n",
      "302 M     Total params\n",
      "1,210.960 Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "326       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84405f6595f94075820f962b43f98e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:251: You requested to overfit but enabled train dataloader shuffling. We are turning off the train dataloader shuffling for you.\n",
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/thamolwanp/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3113ba7e30462db3930b5170092649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gen_time_llm.tsdataset import TimeSeriesDataset, TimeSeriesDataModule\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')\n",
    "dataset = TimeSeriesDataset.from_jsonl('data/processed_train_data.jsonl', tokenizer=tokenizer)\n",
    "\n",
    "# Instantiate the model with the trial's hyperparameters\n",
    "model = TimeLLM(\n",
    "    input_keys=['temporal_series', 'sector', 'country'],\n",
    "    random_seed=42,\n",
    "    input_size=218,\n",
    "    base_lr=1e-3\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    overfit_batches=1,\n",
    "    accelerator='cpu'\n",
    ")\n",
    "\n",
    "# Assuming dataset is an instance of your TimeSeriesDataset\n",
    "total_size = len(dataset)\n",
    "\n",
    "# Define the lengths for training and validation sets\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create a TimeSeriesDataModule using the given datasets and tokenizer\n",
    "datamodule = TimeSeriesDataModule(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,  # Use the same tokenizer\n",
    "    batch_size=4,\n",
    "    valid_batch_size=4\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
