"""Torch Dataset for Time Series"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/tsdataset.ipynb.

# %% auto 0
__all__ = ['sector_column_mapping', 'LengthBasedBatchSampler', 'TimeSeriesLoader', 'TimeSeriesDataset', 'TimeSeriesDataModule']

# %% ../nbs/tsdataset.ipynb 4
import warnings
import re
import torch
import json
from collections.abc import Mapping
from torch.utils.data import Dataset, DataLoader, Sampler
import pytorch_lightning as pl

# %% ../nbs/tsdataset.ipynb 5
class LengthBasedBatchSampler(Sampler):
    def __init__(self, data_source, batch_size, sort_key='summary_input_ids'):
        self.data_source = data_source
        self.batch_size = batch_size
        self.sort_key = sort_key

        # Sort indices by the length of `sort_key`
        self.sorted_indices = sorted(range(len(data_source)), key=lambda i: len(data_source[i][sort_key]))

    def __iter__(self):
        # Generate batches from sorted indices
        batches = [self.sorted_indices[i:i + self.batch_size] for i in range(0, len(self.sorted_indices), self.batch_size)]
        return iter(batches)

    def __len__(self):
        return (len(self.data_source) + self.batch_size - 1) // self.batch_size

# %% ../nbs/tsdataset.ipynb 6
class TimeSeriesLoader(DataLoader):
    """TimeSeriesLoader DataLoader.
    
    Custom DataLoader to work with time series datasets, handling dynamic padding for tokenized summaries and attention masks,
    and using the tokenizer's `eos_token_id` for padding.
    """
    
    def __init__(self, dataset, tokenizer, **kwargs):
        """
        Initializes the loader with the dataset and tokenizer.
        
        Parameters:
        - dataset: The TimeSeriesDataset instance.
        - tokenizer: The tokenizer used for tokenizing summaries (e.g., from HuggingFace's Transformers library).
        """
        self.tokenizer = tokenizer  # Store the tokenizer for eos_token_id
        if 'collate_fn' in kwargs:
            kwargs.pop('collate_fn')
        kwargs_ = {**kwargs, **dict(collate_fn=self._collate_fn)}
        super().__init__(dataset=dataset, **kwargs_)
    
    def _collate_fn(self, batch):
        """
        Custom collate function to handle time series data and dynamically pad tokenized summaries with `eos_token_id`.
        """
        elem = batch[0]
        elem_type = type(elem)

        # Handle case when the batch is a tensor (e.g., temporal series)
        if isinstance(elem, torch.Tensor):
            return torch.stack(batch, dim=0)

        # Handle case when the batch is a dictionary
        elif isinstance(elem, Mapping):
            # Collate temporal series (stack 2D time series tensors)
            temporal_series = self.collate_fn([d['temporal_series'] for d in batch])
            
            # Collate sector information (as a list)
            sector = [d['sector'] for d in batch]
            
            # Find the maximum sequence length in the current batch for dynamic padding
            max_length = max([d['summary_input_ids'].size(0) for d in batch])
            
            # Dynamically pad summaries using eos_token_id
            eos_token_id = self.tokenizer.eos_token_id
            summary_input_ids = torch.stack([torch.cat([d['summary_input_ids'], 
                                                        torch.full((max_length - d['summary_input_ids'].size(0),), 
                                                                   eos_token_id, dtype=torch.long)])
                                             for d in batch])
            
            # Dynamically pad attention masks (using 0 for padding)
            attention_mask = None
            if batch[0]['attention_mask'] is not None:
                attention_mask = torch.stack([torch.cat([d['attention_mask'], 
                                                         torch.zeros(max_length - d['attention_mask'].size(0), 
                                                                     dtype=torch.long)])
                                              for d in batch])
            
            # Collate country information (keeping as list of strings)
            country = [d['country'] for d in batch]
            
            # Collate columns of temporal data (should remain consistent across batch)
            temporal_cols = batch[0]['temporal_cols']

            year_range = [d['year_range'] for d in batch]

            col_indices = [d['col_indices'] for d in batch]

            # Return the collated batch with dynamic padding for tokenized summaries
            return dict(
                temporal_series=temporal_series,
                sector=sector,
                summary_input_ids=summary_input_ids,
                attention_mask=attention_mask,
                country=country,
                temporal_cols=temporal_cols,
                year_range=year_range
            )

        # Raise error if an unsupported data type is passed
        raise TypeError(f'Unknown type {elem_type}')

# %% ../nbs/tsdataset.ipynb 8
sector_column_mapping = {
    "Energy": [
        'annual_change_in_coal_production__twh', 'annual_change_in_gas_production__twh',
        'annual_change_in_oil_production__twh', 'bioenergy__mtco2', 'coal_production__twh',
        'gas_production_per_capita__kwh', 'hydro_generation__twh', 'nuclear__twh__direct',
        'primary_energy_consumption__twh', 'solar_and_wind__twh__direct'
    ],
    "LULUCF": [
        'ag_lnd_agri_k2', 'ag_lnd_arbl_zs', 'forest', 'pasture_area_change', 'cropland_area_change',
        'agriculture_area_change', 'land_use_change_and_forestry_ch4_emissions_per_capita',
        'land_use_change_and_forestry_co2_emissions', 'land_use_change_and_forestry_ghg_emissions',
        'land_use_change_and_forestry_n2o_emissions_per_capita'
    ],
    "Buildings": [
        'buildings_ch4_emissions_per_capita', 'buildings_co2_emissions_per_capita',
        'buildings_ghg_emissions_per_capita', 'buildings_n2o_emissions_per_capita',
        'population_density', 'cement_co2_per_capita', 'co2_per_capita'
    ],
    "Economy-wide": [
        'population', 'population_density', 'co2_growth_prct', 'ny_gdp_frst_rt_zs', 'ny_gdp_mktp_kd_zg',
        'co2_with_tax_as_share_of_co2', 'co2_intensity__gco2_kwh', 'sp_pop_grow', 'energy_cons_change_twh'
    ],
    "Water": [
        'amount_of_water__and_sanitation_related_official_development_assistance_that_is_part_of_a_government_coordinated_spending_plan__current_usd_millions',
        'nlis__population_using_improved_drinking_water_sources__pct', 'population_using_at_least_basic_drinking_water_services__pct',
        'population_using_safely_managed_drinking_water_services__pct', 'water__sanitation_and_hygiene_attributable_dalys__000',
        'water__sanitation_and_hygiene_attributable_dalys_per_100000_capita'
    ],
    "Urban": [
        'degurba_l1_population_city_estimates', 'degurba_l1_population_city_share_estimates',
        'degurba_l1_population_town__and__suburbs_share_estimates', 'degurba_l2_population_dense_town_share_estimates',
        'degurba_l2_population_semi_dense_town_share_estimates', 'share_of_urban_population_owid_estimates'
    ],
    "Transport": [
        'aviation_and_shipping_ch4_emissions_per_capita', 'aviation_and_shipping_co2_emissions',
        'aviation_and_shipping_ghg_emissions_per_capita', 'registered_vehicles_per_thousand',
        'transport_ch4_emissions_per_capita', 'number_of_registered_vehicles', 'coal_production_per_capita__kwh'
    ],
    "Public Sector": [
        'compliance_with_smoke_free_public_transport_law__score', 'public_expenditure_on_education__tanzi__and__schuktnecht__2000',
        'democracy_satisf_claassen', 'democracy_support_claassen', 'democracy_bti', 'exec_recopen_polity'
    ],
    "Health": [
        'cantril_ladder_score', 'life_expectancy', 'illiterate', 'literacy_rates__world_bank__cia_world_factbook__and_other_sources',
        'suffering_index', 'struggling_index', 'years_of_education_gini'
    ],
    "Environment": [
        'co2_with_ets_as_share_of_co2', 'co2_with_tax_as_share_of_co2', 'concentrations_of_fine_particulate_matter__pm2_5',
        'plastic_waste', 'sdg_6_3_1_proportion_of_safely_treated_domestic_wastewater_flows__pct',
        'total_emissions__mtco2', 'population_using_at_least_basic_drinking_water_services__pct'
    ],
    "Agriculture": [
        'agriculture_area', 'agriculture_area_change', 'cereal_yield', 'cereals_yield', 'fruit_yield',
        'tomato_yield', 'roots_and_tubers_yield', 'agriculture_ch4_emissions', 'agriculture_n2o_emissions',
        'pulses_yield'
    ],
    "Industry": [
        'industry_ch4_emissions_per_capita', 'industry_co2_emissions_per_capita', 'industry_fgas_emissions',
        'industry_ghg_emissions_per_capita', 'manufacturing_and_construction_ch4_emissions_per_capita',
        'manufacturing_and_construction_n2o_emissions_per_capita', 'industry_n2o_emissions_per_capita'
    ],
    "Tourism": [
        'tourism_share_gdp', 'in_tour_regions_americas', 'in_tour_regions_europe', 'import_total_mot',
        'import_total_mot_per_capita', 'net_export', 'net_export_per_capita'
    ],
    "Waste": [
        'waste_ch4_emissions_per_capita', 'waste_ghg_emissions_per_capita', 'waste_n2o_emissions_per_capita',
        'total_emissions__mtco2', 'sdg_14_40_pct_ex_c', 'sdg_14_40_nr_in'
    ],
    "Coastal zones": [
        'average_area_km2_estimates', 'ifl_area', 'total_area_km2_estimates', 'co2_intensity__gco2_kwh',
        'concentrations_of_fine_particulate_matter__pm2_5'
    ],
    "Cross Cutting Area": [
        'population_with_basic_education', 'population_using_safely_managed_drinking_water_services__pct',
        'population_using_at_least_basic_drinking_water_services__pct', 'mf_youth_and_adults__15_64_years__percentage_of_no_education',
        'primary_energy_consumption__twh', 'total_area_km2_estimates', 'nlis__population_using_improved_drinking_water_sources__pct'
    ],
    "Disaster Risk Management (Drm)": [
        'regime_dich_ert', 'regime_trich_ert', 'struggling_index', 'suffering_index', 'average_area_km2_estimates'
    ],
    "Finance": [
        'ny_gdp_mktp_kd_zg', 'ny_adj_aedu_gn_zs', 'ny_adj_dmin_cd', 'nlis__population_using_improved_drinking_water_sources__pct',
        'amount_of_water__and_sanitation_related_official_development_assistance_that_is_part_of_a_government_coordinated_spending_plan__current_usd_millions'
    ],
    "Rural": [
        'sp_rur_totl_zg', 'sp_rur_totl_zs', 'agriculture_area_change', 'population_density', 'population'
    ],
    "Social development": [
        'democracy_satisf_claassen', 'democracy_support_claassen', 'no_formal_education', 'mf_tertiary_enrollment_rates_combined_wb',
        'mf_youth_and_adults__15_64_years__average_years_of_education', 'democracy_bti', 'share'
    ],
    "Adaptation": [
        'energy_cons_change_pct', 'energy_cons_change_twh', 'primary_energy_consumption__twh',
        'population_density', 'agriculture_ghg_emissions'
    ],
    "Residential and Commercial": [
        'buildings_co2_emissions_per_capita', 'buildings_ch4_emissions_per_capita', 'buildings_n2o_emissions_per_capita',
        'buildings_ghg_emissions_per_capita', 'registered_vehicles_per_thousand', 'population_density'
    ]
}

# %% ../nbs/tsdataset.ipynb 9
class TimeSeriesDataset(Dataset):
    def __init__(self,
                 data_list,  # List of dictionaries containing time series and metadata
                 tokenizer,  # Tokenizer for summarizing text (e.g., from HuggingFace's Transformers library)
                 max_length: int = 512,  # Max token length for tokenization
                 sorted=False,  # Whether the dataset is already sorted
                 add_attention_mask: bool = True  # Whether to include attention mask for tokenized summaries
                ):
        """
        A dataset class for structured time series data, with both temporal and static (text) features.
        
        Parameters:
        - data_list: List of dictionaries, where each dictionary contains keys like:
            - 'anchor_summary': Short description or metadata (to be tokenized).
            - 'positive_time_series': 2D array of temporal data for the entity.
            - 'positive_sector': One-hot encoded sector information.
            - 'sector': Sectors assigned to this time series (as string).
            - 'country': Country associated with the time series.
            - 'columns': Names of the temporal columns/features.
        - tokenizer: Tokenizer instance for encoding the summaries (e.g., GPT tokenizer or any other transformer model).
        - max_length: Maximum length for the tokenized summaries (default: 512).
        - sorted: Whether the dataset is already sorted (default: False).
        - add_attention_mask: Whether to add an attention mask for tokenized summaries (default: True).
        """
        super().__init__()
        
        self.data_list = data_list
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.sorted = sorted
        self.add_attention_mask = add_attention_mask
        
        # Filter out data entries with tokenized summary lengths < 100
        self.data_list = [
            data for data in data_list 
            if len(tokenizer(data['anchor_summary'], max_length=max_length, truncation=True)['input_ids']) >= 100
        ]

        self.n_groups = len(self.data_list)  # Update the count after filtering

    def clean_text(self, text):
        # Remove duplicate spaces and newlines
        text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
        text = text.replace('\n', ' ')  # Replace newlines with a space
        text = text.strip()  # Remove leading and trailing spaces
        return text

    def __len__(self):
        """
        Return the number of time series entities in the dataset.
        """
        return self.n_groups
    
    def __getitem__(self, idx):
        """
        Return a single item from the dataset (time series and its metadata).
        The index `idx` specifies which time series entity to retrieve.
        """
        data = self.data_list[idx]
        
        # Extract fields from the dictionary
        temporal_series = torch.tensor(data['positive_time_series'], dtype=torch.float32)
        anchor_summary = self.clean_text(data['anchor_summary'])
        country = data['country']
        columns = data['columns']
        sector_str = data['sector']
        year_range = data['year_range']

        # Retrieve column indices related to all sectors in sector_list
        column_indices = set()
        for sector in sector_str:
            sector_columns = sector_column_mapping.get(sector, [])
            for col in sector_columns:
                if col in columns:
                    column_indices.add(columns.index(col))

        # Convert the set of indices to a sorted list
        column_indices = sorted(list(column_indices))

        # Manually add the BOS and EOS tokens to the input summary
        # bos_token = self.tokenizer.bos_token or self.tokenizer.cls_token  # Default to CLS if BOS isn't defined
        eos_token = self.tokenizer.eos_token or self.tokenizer.sep_token  # Default to SEP if EOS isn't defined

        # Concatenate the EOS tokens to the summary
        anchor_summary_with_eos = anchor_summary + " " + eos_token

        # Tokenize the summary with the specified tokenizer
        tokenized_summary = self.tokenizer(
            anchor_summary_with_eos,
            max_length=self.max_length,
            truncation=True,
            return_tensors='pt'  # Return PyTorch tensors
        )


        # Extract tokenized input_ids and attention mask (optional)
        input_ids = tokenized_summary['input_ids'].squeeze(0)  # Remove batch dimension
        attention_mask = tokenized_summary['attention_mask'].squeeze(0) if self.add_attention_mask else None

        # Return a dictionary with both temporal and static features, including tokenized summary
        return {
            'temporal_series': temporal_series,  # 2D time series data
            'sector': sector_str,                # Sectors as string
            'summary_input_ids': input_ids,      # Tokenized summary
            'attention_mask': attention_mask,    # Attention mask (if applicable)
            'country': country,                  # Static feature (country)
            'temporal_cols': columns,            # Names of temporal features
            'year_range': year_range,
            'col_indices': column_indices
        }
    
    def __repr__(self):
        """
        Return a string representation of the dataset, showing the number of data points and groups.
        """
        return f"TimeSeriesDataset(n_data={len(self.data_list):,}, n_groups={self.n_groups:,})"

    def __eq__(self, other):
        """
        Check if two datasets are equal by comparing their data and attributes.
        """
        if not isinstance(other, TimeSeriesDataset):
            return False
        return (
            self.data_list == other.data_list and
            self.max_length == other.max_length and
            self.sorted == other.sorted
        )
    
    @staticmethod
    def from_jsonl(file_path, tokenizer, max_length=512, sorted=False, add_attention_mask=True):
        """
        Static method to load time series data from a JSONL file.
        
        Parameters:
        - file_path: Path to the JSONL file.
        - tokenizer: Tokenizer to use for tokenizing the summaries.
        - max_length: Maximum token length for the summaries.
        - sorted: Whether the dataset should be sorted.
        - add_attention_mask: Whether to include attention masks for tokenized summaries.

        Returns:
        - dataset: TimeSeriesDataset instance with loaded data.
        """
        # Load the JSONL file
        data_list = []
        with open(file_path, 'r') as f:
            for line in f:
                data_list.append(json.loads(line))

        # Create and return the dataset instance
        return TimeSeriesDataset(
            data_list=data_list,
            tokenizer=tokenizer,
            max_length=max_length,
            sorted=sorted,
            add_attention_mask=add_attention_mask
        )

# %% ../nbs/tsdataset.ipynb 12
class TimeSeriesDataModule(pl.LightningDataModule):
    
    def __init__(
            self, 
            train_dataset: TimeSeriesDataset,  # Separate dataset for training
            val_dataset: TimeSeriesDataset,    # Separate dataset for validation
            tokenizer,                         # Tokenizer for all datasets
            batch_size=32, 
            valid_batch_size=8,
            num_workers=0,
            drop_last=False,
            shuffle_train=True,
            test_dataset: TimeSeriesDataset = None,   # Separate dataset for testing (optional)
        ):
        """
        A DataModule for loading time series data, supporting training, validation, and prediction.
        
        Parameters:
        - train_dataset: The TimeSeriesDataset instance for the training data.
        - val_dataset: The TimeSeriesDataset instance for the validation data.
        - test_dataset: The TimeSeriesDataset instance for the test data (optional).
        - tokenizer: The tokenizer used for tokenizing summaries (e.g., from HuggingFace's Transformers library).
        - batch_size: Batch size for the training data.
        - valid_batch_size: Batch size for the validation and test data.
        - num_workers: Number of workers for data loading (default: 0).
        - drop_last: Whether to drop the last incomplete batch (default: False).
        - shuffle_train: Whether to shuffle the training data (default: True).
        """
        super().__init__()
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.test_dataset = test_dataset
        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.valid_batch_size = valid_batch_size
        self.num_workers = num_workers
        self.drop_last = drop_last
        self.shuffle_train = shuffle_train

        self.tokenizer.pad_token = self.tokenizer.eos_token  # Ensure padding token is set
    
    def train_dataloader(self):
        """
        Creates and returns a DataLoader for the training dataset.
        """
        # sampler = LengthBasedBatchSampler(self.train_dataset, batch_size=self.batch_size, sort_key='summary_input_ids')
        loader = TimeSeriesLoader(
            self.train_dataset,
            tokenizer=self.tokenizer,  # Pass the tokenizer
            batch_size=self.batch_size, 
            num_workers=self.num_workers,
            shuffle=self.shuffle_train,
            # batch_sampler=sampler,
            drop_last=self.drop_last
        )
        # loader = TimeSeriesLoader(
        #     self.train_dataset,
        #     tokenizer=self.tokenizer,
        #     num_workers=self.num_workers,
        #     batch_sampler=sampler
        # )
        return loader
    
    def val_dataloader(self):
        """
        Creates and returns a DataLoader for the validation dataset.
        """
        loader = TimeSeriesLoader(
            self.val_dataset, 
            tokenizer=self.tokenizer,  # Pass the tokenizer
            batch_size=self.valid_batch_size, 
            num_workers=self.num_workers,
            shuffle=False,
            drop_last=self.drop_last
        )
        return loader
    
    def test_dataloader(self):
        """
        Creates and returns a DataLoader for the test dataset.
        """
        if self.test_dataset:
            loader = TimeSeriesLoader(
                self.test_dataset,
                tokenizer=self.tokenizer,  # Pass the tokenizer
                batch_size=self.valid_batch_size, 
                num_workers=self.num_workers,
                shuffle=False
            )
            return loader
        return None
