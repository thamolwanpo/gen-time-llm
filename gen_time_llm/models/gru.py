# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.gru.ipynb.

# %% auto 0
__all__ = ['GRUGPTModel']

# %% ../../nbs/models.gru.ipynb 3
import torch
import torch.nn as nn
import pytorch_lightning as pl
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import optuna
from optuna.trial import Trial
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

from ..common._base_model import BaseModel

# %% ../../nbs/models.gru.ipynb 4
class GRUGPTModel(BaseModel):
    """
    Model combining a GRU encoder for time series and a GPT-based decoder for text generation,
    with temporal normalization/scaling.
    """

    def __init__(
        self,
        random_seed,
        loss,  # Loss function for training
        tokenizer,  # Tokenizer for decoding generated text
        hidden_size=256,  # Hidden size of the GRU encoder
        num_layers=4,  # Number of GRU layers
        base_lr=1e-5,  # Learning rate
        max_length=512,  # Maximum length of generated sequences
        num_beams=3,  # Number of beams for beam search
        gru_input_size=128,  # Size of the input for the GRU (e.g., number of features in the time series)
        **kwargs
    ):
        super(GRUGPTModel).__init__(
            random_seed=random_seed,
            loss=loss,
            tokenizer=tokenizer,
            max_length=max_length,
            num_beams=num_beams,
            **kwargs
        )

        self.tokenizer = tokenizer
        self.max_length = max_length

        # GRU Encoder
        self.gru = nn.GRU(
            input_size=gru_input_size,  # Number of input features in the time series
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )

        # GPT Decoder
        self.gpt = GPT2LMHeadModel.from_pretrained("gpt2")
        # Freeze the GPT model parameters
        for param in self.gpt.parameters():
            param.requires_grad = False

        # Put the GPT model in evaluation mode
        self.gpt.eval()  # Ensure GPT is in evaluation mode

        # Mapping GRU hidden state to the GPT's embedding size
        self.hidden_to_gpt = nn.Linear(hidden_size, self.gpt.config.n_embd)

        # Learning rate
        self.base_lr = base_lr

    def forward(self, batch, targets=None, use_teacher_forcing=False):
        """
        Forward pass of the model.
        - time_series: Time series input (batch_size, seq_length, num_features)
        - targets: Target text used for teacher forcing (optional)
        - use_teacher_forcing: Boolean flag for using teacher forcing
        Returns:
        - gpt_output.loss if using teacher forcing
        - gpt_output logits if autoregressive generation
        """
        inputs = {key: batch[key] for key in self.input_keys}
        time_series = inputs['temporal_series']

        # GRU encoding
        _, hidden_state = self.gru(time_series)
        hidden_state = hidden_state[-1]

        # Map hidden state to GPT's input size (this is the time series representation)
        gpt_input = self.hidden_to_gpt(hidden_state).unsqueeze(1)  # (batch_size, 1, gpt_hidden_size)

        if use_teacher_forcing and targets is not None:
            # Teacher forcing: pass inputs and labels to GPT for loss computation
            gpt_input_ids = targets[:, :-1]  # Input part of the target sequence (ignore last token)
            token_embeddings = self.gpt.transformer.wte(gpt_input_ids)
            gpt_input_combined = torch.cat([gpt_input, token_embeddings], dim=1)
            
            # Pass to GPT and compute loss directly
            gpt_output = self.gpt(inputs_embeds=gpt_input_combined, labels=targets)
            return gpt_output.loss

        else:
            # Autoregressive generation with past_key_values management
            outputs = []

            for _ in range(self.max_length):
                # Generate the next token with time series embedding (gpt_input) and past_key_values
                gpt_output = self.gpt(inputs_embeds=gpt_input)
                logits = gpt_output.logits[:, -1, :]
                outputs.append(logits.unsqueeze(1))

                next_token = torch.argmax(logits, dim=-1)
                gpt_input = self.gpt.transformer.wte(next_token).unsqueeze(1)

            outputs = torch.cat(outputs, dim=1)  # Concatenate the outputs along sequence dimension

        return outputs

    def generate(self, time_series, max_length=None, num_beams=3):
      """
      Generate text from time series data using autoregressive generation and beam search.
      """
      max_length = max_length if max_length is not None else self.max_length

      # GRU encoding
      _, hidden_state = self.gru(time_series)
      hidden_state = hidden_state[-1]  # Get the last layer's hidden state (batch_size, hidden_size)

      # Map hidden state to GPT's input size
      gpt_input = self.hidden_to_gpt(hidden_state)

      # Start token (e.g., <BOS> or a special token depending on your tokenizer)
      start_token = self.tokenizer.bos_token_id if self.tokenizer.bos_token_id else self.tokenizer.eos_token_id
      input_ids = torch.full((time_series.size(0), 1), start_token, dtype=torch.long, device=self.device)

      # Pass the hidden state to GPT as embeddings (initial hidden state for generation)
      inputs_embeds = gpt_input.unsqueeze(1)  # (batch_size, 1, hidden_dim)

      # Generate text using the built-in generate() function from transformers
      generated_ids = self.gpt.generate(
          inputs_embeds=inputs_embeds,  # Use the hidden state as input embeddings
          max_length=max_length,
          num_beams=num_beams,
          early_stopping=True,  # Stop when all beams generate the <EOS> token
          no_repeat_ngram_size=2,  # Optionally prevent repetition
      )

      # Decode the generated token IDs back into text
      generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

      return generated_text


    def configure_optimizers(self):
        """
        Configure optimizers and learning rate scheduler.
        """
        optimizer = torch.optim.Adam(self.parameters(), lr=self.base_lr)
        return optimizer
